<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> 
#### [Generating Synthetic Decentralized Social Graphs with Local Differential Privacy](https://acmccs.github.io/papers/p425-qinAemb.pdf)

#### background & previous work

- a large amount of valuable info residues in decentralized social graphs, one could not simply reconstruct a decentralized social network due to privacy concerns
- existing solutions publish graph data or analysis results under certain privacy guarantees, such as differential privacy
- more challenging for decentralized social graph due to no party can access to the whole graph

#### our work

- investigate techniques to ensure local differential privacy of individuals, show they're insufficient for preserving import graph properties
- propose LDPGen, a novel multi-phase technique, which captures the structure of origin, incrementally clusters users based on their connections, construct a synthetic social graph
- LDPGen produces high-quality synthetic graphs that those from baseline approaches
- Experiments in real: 
  - statistical analysis
  - community discovery
  - social recommendation
- main contributions:
  - formulate the problem under LDP
  - analyze existing work
  - propose LDPGen
  - experiments in real
- Node LDP as future work

#### LDP

- $\frac{Pr[M(D)=s]}{Pr[M(D')=s]}\le e^\epsilon$, here M is a randomize mechanism, D is a database, attacker cannot infer output with high confidence whether input is D or D'
  - Node local differential privacy(NLDP)
  - Edge local differential privacy(ELDP)
  - NLDP is stronger that ELDP
- Randomized Neighbor List Approach(RNL) is Edge LDP, but return is denser
- Degree-based Graph Generation Approach

#### LPDGEN

- strike a good balance between noise & info loss--RNL incurs excessive noise whereas DGG incurs excessive info loss
- three phases: initial grouping ->grouping refinement->graph generation
- Phase 1: define the objective function as $E[\frac{|\tilde{d}-d|}{d}] \le E[\frac{\tilde{d}-\hat{d}}{d}]+E[\frac{\hat{d}-d}{d}]\le\frac{1}{d}(\frac{2k_1}{\epsilon_2}+d-E[\hat{d}])$ , therefore we focus on $E[\hat{d}]=\sum_{t=0}^dt\frac{\theta_{\hat{d}=t}}{\theta_d}$. explain the following formula by mathematical language: ![](9-1.PNG)

  - the number of ''cancelling'' neighbor pairs is $\frac{d-t}{2}$:  suppose there are $x$ cancelling neighbor pairs and $y$ non-cancelling, $z$ common neighbors, we have $x+y=d,y=\frac{t}{2}$, then $x=\frac{d-t}{2}$
  - the number of ways to select $\frac{d-t}{2}$ neighbors from each user for the further pairing is ${d/2\choose t/2}^2$: notice that it is equivalent to calculate the number of ways to select $\frac{t}{2}$ neighbors.
  - the number of ways to pair these cancelling neighbors from two users is $\frac{d-t}{2}!$, and there are $d_1^{\frac{d-t}{2}}$ ways to allocate them among $k_1$ groups: it's obvious.
  - the number of node pairs allocated to different groups is $\frac{t}{2}$:  it's trivial.
  - there are ${k_1\choose 2}^{t/2}$ ways to allocate these pairs among $k_1$ groups: each non-cancelling pairs choose 2 different groups from $k_1$ groups, do it for $t/2$ rounds.
  - combination: 

    - first we choose $\frac{d-t}{2}$ neighbors from each user: ${d/2\choose t/2}^2$
    - pair them: $\frac{d-t}{2}!$
    - allocate them among $k_1$ groups: $k_1^{\frac{d-t}{2}}$
    - pair $\frac{t}{2}$ non-cancelling neighbors(we don't care how to choose): $\frac{t}{2}!$
    - allocate them: ${k_1\choose 2}^{t/2}$

  - we get a good approximated closed form for ![](9-2.PNG), minimized it to get $k_1$.
- Phase 2: refine the clustering of users, the number of target clusters is still $k_1$(think: how to get clusters?)
- Phase 3: similar to Chung-Lu model, compute the prob. to connect two node

#### Privacy Analysis & Complexity

- $x,y\sim\mathrm{Lap}(0,\frac{1}{\epsilon})$, and $|x-y|=1$, then $\frac{p(x,x+dx)}{p(x\pm1,x\pm1+dx)}=f(x)/f(x\pm 1)=e^{\pm\epsilon}\le e^\epsilon$, here $f(x)$ is the prob. density function.
- two rounds of communication, higher than stra man approach
- O(n^2+n(k0+k1)), slightly higher than straw man methods

***

- Laplacian distribution
- grouping refinement

</script>